{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short version**:\n",
    "\n",
    "Suppose you have two tensors, where `y_hat` contains computed scores for each class (for example, from y = W*x +b) and `y_true` contains one-hot encoded true labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat  = ... # Predicted label, e.g. y = tf.matmul(X, W) + b\n",
    "y_true = ... # True label, one-hot encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you interpret the scores in `y_hat` as unnormalized log probabilities, then they are **logits**.\n",
    "\n",
    "Additionally, the total cross-entropy loss computed in this manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_softmax = tf.nn.softmax(y_hat)\n",
    "total_loss = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_hat_softmax), [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is essentially equivalent to the total cross-entropy loss computed with the function `softmax_cross_entropy_with_logits()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Long version:**\n",
    "\n",
    "In the output layer of your neural network, you will probably compute an array that contains the class scores for each of your training instances, such as from a computation `y_hat = W*x + b`. To serve as an example, below I've created a `y_hat` as a 2 x 3 array, where the rows correspond to the training instances and the columns correspond to classes. So here there are 2 training instances and 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Create example y_hat.\n",
    "y_hat = tf.convert_to_tensor(np.array([[0.5, 1.5, 0.1],[2.2, 1.3, 1.7]]))\n",
    "sess.run(y_hat)\n",
    "# array([[ 0.5,  1.5,  0.1],\n",
    "#        [ 2.2,  1.3,  1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the values are not normalized (i.e. the rows don't add up to 1). In order to normalize them, we can apply the softmax function, which interprets the input as unnormalized log probabilities (aka **logits**) and outputs normalized linear probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_softmax = tf.nn.softmax(y_hat)\n",
    "sess.run(y_hat_softmax)\n",
    "# array([[ 0.227863  ,  0.61939586,  0.15274114],\n",
    "#        [ 0.49674623,  0.20196195,  0.30129182]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to fully understand what the softmax output is saying. Below I've shown a table that more clearly represents the output above. It can be seen that, for example, the probability of training instance 1 being \"Class 2\" is 0.619. The class probabilities for each training instance are normalized, so the sum of each row is 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                          Pr(Class 1)  Pr(Class 2)  Pr(Class 3)\n",
    "                        ,--------------------------------------\n",
    "    Training instance 1 | 0.227863   | 0.61939586 | 0.15274114\n",
    "    Training instance 2 | 0.49674623 | 0.20196195 | 0.30129182\n",
    "    \n",
    "    \n",
    "So now we have class probabilities for each training instance, where we can take the argmax() of each row to generate a final classification. From above, we may generate that training instance 1 belongs to \"Class 2\" and training instance 2 belongs to \"Class 1\". \n",
    "\n",
    "Are these classifications correct? We need to measure against the true labels from the training set. You will need a one-hot encoded `y_true` array, where again the rows are training instances and columns are classes. Below I've created an example `y_true` one-hot array where the true label for training instance 1 is \"Class 2\" and the true label for training instance 2 is \"Class 3\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.convert_to_tensor(np.array([[0.0, 1.0, 0.0],[0.0, 0.0, 1.0]]))\n",
    "sess.run(y_true)\n",
    "# array([[ 0.,  1.,  0.],\n",
    "#        [ 0.,  0.,  1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the probability distribution in `y_hat_softmax` close to the probability distribution in `y_true`? We can use cross-entropy loss to measure the error.\n",
    "\n",
    "\\begin{equation}\n",
    "H(p, q) = - \\sum_{x} p(x) \\log [q(x)]\n",
    "\\end{equation}\n",
    "\n",
    "We can compute the cross-entropy loss on a row-wise basis and see the results. Below we can see that training instance 1 has a loss of 0.479, while training instance 2 has a higher loss of 1.200. This result makes sense because in our example above, `y_hat_softmax` showed that training instance 1's highest probability was for \"Class 2\", which matches training instance 1 in `y_true`; however, the prediction for training instance 2 showed a highest probability for \"Class 1\", which does not match the true class \"Class 3\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_instance_1 = -tf.reduce_sum(y_true * tf.log(y_hat_softmax), reduction_indices=[1])\n",
    "sess.run(loss_per_instance_1)\n",
    "# array([ 0.4790107 ,  1.19967598])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we really want is the total loss over all the training instances. So we can compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_1 = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_hat_softmax), reduction_indices=[1]))\n",
    "sess.run(total_loss_1)\n",
    "# 0.83934333897877944"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using softmax_cross_entropy_with_logits()**\n",
    "\n",
    "We can instead compute the total cross entropy loss using the `tf.nn.softmax_cross_entropy_with_logits()` function, as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_instance_2 = tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true)\n",
    "sess.run(loss_per_instance_2)\n",
    "# array([ 0.4790107 ,  1.19967598])\n",
    "\n",
    "total_loss_2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true))\n",
    "sess.run(total_loss_2)\n",
    "# 0.83934333897877922"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `total_loss_1` and `total_loss_2` produce essentially equivalent results with some small differences in the very final digits. However, you might as well use the second approach: it takes one less line of code and accumulates less numerical error because the softmax is done for you inside of `softmax_cross_entropy_with_logits()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid functions family\n",
    "\n",
    "1. tf.nn.sigmoid_cross_entropy_with_logits\n",
    "2. tf.nn.weighted_cross_entropy_with_logits\n",
    "3. tf.losses.sigmoid_cross_entropy\n",
    "4. tf.contrib.losses.sigmoid_cross_entropy (DEPRECATED)\n",
    "\n",
    "As stated earlier, sigmoid loss function is for binary classification. But tensorflow functions are more general and allow to do multi-label classification, when the classes are independent. In other words, tf.nn.sigmoid_cross_entropy_with_logits solves N binary classifications at once.\n",
    "\n",
    "The labels must be one-hot encoded or can contain soft class probabilities.\n",
    "\n",
    "tf.losses.sigmoid_cross_entropy in addition allows to set the in-batch weights, i.e. make some examples more important than others. tf.nn.weighted_cross_entropy_with_logits allows to set class weights (remember, the classification is binary), i.e. make positive errors larger than negative errors. This is useful when the training data is unbalanced.\n",
    "\n",
    "# Softmax functions family\n",
    "\n",
    "1. tf.nn.softmax_cross_entropy_with_logits (DEPRECATED IN 1.5)\n",
    "2. tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "3. tf.losses.softmax_cross_entropy\n",
    "4. tf.contrib.losses.softmax_cross_entropy (DEPRECATED)\n",
    "\n",
    "For softmax_cross_entropy_with_logits, labels must have the shape [batch_size, num_classes] and dtype float32 or float64.\n",
    "Labels used in softmax_cross_entropy_with_logits are the one hot version of labels used in sparse_softmax_cross_entropy_with_logits. So, to make it work using softmax_cross_entropy_with_logits, you will have to do tf.one_hot on the labels.\n",
    "\n",
    "These loss functions should be used for multinomial mutually exclusive classification, i.e. pick one out of N classes. Also applicable when N = 2.\n",
    "\n",
    "The labels must be one-hot encoded or can contain soft class probabilities: a particular example can belong to class A with 50% probability and class B with 50% probability. Note that strictly speaking it doesn't mean that it belongs to both classes, but one can interpret the probabilities this way.\n",
    "\n",
    "Just like in sigmoid family, tf.losses.softmax_cross_entropy allows to set the in-batch weights, i.e. make some examples more important than others. As far as I know, as of tensorflow 1.3, there's no built-in way to set class weights.\n",
    "\n",
    "In tensorflow 1.5, v2 version was introduced and the original softmax_cross_entropy_with_logits loss got deprecated. The only difference between them is that in a newer version, backpropagation happens into both logits and labels.\n",
    "\n",
    "In supervised learning one doesn't need to backpropagate to labels. They are considered fixed ground truth and only the weights need to be adjusted to match them.\n",
    "\n",
    "But in some cases, the labels themselves may come from a differentiable source, another network. One example might be adversarial learning. In this case, both networks might benefit from the error signal. That's the reason why tf.nn.softmax_cross_entropy_with_logits_v2 was introduced. Note that when the labels are the placeholders (which is also typical), there is no difference if the gradient through flows or not, because there are no variables to apply gradient to.\n",
    "\n",
    "# Sparse functions family\n",
    "\n",
    "1. tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "2. tf.losses.sparse_softmax_cross_entropy\n",
    "3. tf.contrib.losses.sparse_softmax_cross_entropy (DEPRECATED)\n",
    "\n",
    "For sparse_softmax_cross_entropy_with_logits labels must have the shape [batch_size] and the dtype int32 or int64. Each label is an int in range [0, num_classes-1].\n",
    "\n",
    "Like ordinary softmax above, these loss functions should be used for multinomial mutually exclusive classification, i.e. pick one out of N classes. The difference is in labels encoding: the classes are specified as integers (class index), not one-hot vectors. Obviously, this doesn't allow soft classes, but it can save some memory when there are thousands or millions of classes. However, note that logits argument must still contain logits per each class, thus it consumes at least [batch_size, classes] memory.\n",
    "\n",
    "Like above, tf.losses version has a weights argument which allows to set the in-batch weights\n",
    "\n",
    "# Sampled softmax functions family\n",
    "\n",
    "1. tf.nn.sampled_softmax_loss\n",
    "2. tf.contrib.nn.rank_sampled_softmax_loss\n",
    "3. tf.nn.nce_loss\n",
    "\n",
    "These functions provide another alternative for dealing with huge number of classes. Instead of computing and comparing an exact probability distribution, they compute a loss estimate from a random sample.\n",
    "\n",
    "The arguments weights and biases specify a separate fully-connected layer that is used to compute the logits for a chosen sample.\n",
    "\n",
    "Like above, labels are not one-hot encoded, but have the shape [batch_size, num_true].\n",
    "\n",
    "Sampled functions are only suitable for training. In test time, it's recommended to use a standard softmax loss (either sparse or one-hot) to get an actual distribution.\n",
    "\n",
    "Another alternative loss is tf.nn.nce_loss, which performs noise-contrastive estimation. I've included this function to the softmax family, because NCE guarantees approximation to softmax in the limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class cross-entropy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.nn.softmax_cross_entropy_with_logits directly corresponds to \n",
    "\n",
    "-tf.reduce_sum(p * tf.log(q), axis=1)\n",
    "\n",
    "p and q are expected to be probability distributions over N classes. In particular, N can be 2, as in the following example:\n",
    "\n",
    "################################################################\n",
    "p = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "logit_q = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "q = tf.nn.softmax(logit_q)\n",
    "\n",
    "feed_dict = {\n",
    "  p: [[0, 1],\n",
    "      [1, 0],\n",
    "      [1, 0]],\n",
    "  logit_q: [[0.2, 0.8],\n",
    "            [0.7, 0.3],\n",
    "            [0.5, 0.5]]\n",
    "}\n",
    "\n",
    "prob1 = -tf.reduce_sum(p * tf.log(q), axis=1)\n",
    "prob2 = tf.nn.softmax_cross_entropy_with_logits(labels=p, logits=logit_q)\n",
    "print(prob1.eval(feed_dict))  # [ 0.43748799  0.51301527  0.69314718]\n",
    "print(prob2.eval(feed_dict))  # [ 0.43748799  0.51301527  0.69314718]\n",
    "\n",
    "Note that q is computing tf.nn.softmax, i.e. outputs a probability distribution. \n",
    "So it's still multi-class cross-entropy formula, only for N = 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary cross-entropy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This time the correct formula is\n",
    "\n",
    "p * -tf.log(q) + (1 - p) * -tf.log(1 - q)\n",
    "\n",
    "Though mathematically it's a partial case of the multi-class case, the meaning of p and q is different. In the simplest case, each p and q is a number, corresponding to a probability of the class A.\n",
    "\n",
    "Important: Don't get confused by the common p * -tf.log(q) part and the sum. Previous p was a one-hot vector, now it's a number, zero or one. Same for q - it was a probability distribution, now's it's a number (probability).\n",
    "\n",
    "If p is a vector, each individual component is considered an independent binary classification.\n",
    "\n",
    "So the definition p = [0, 0, 0, 1, 0] doesn't mean a one-hot vector, but 5 different features, 4 of which are off and 1 is on. The definition q = [0.2, 0.2, 0.2, 0.2, 0.2] means that each of 5 features is on with 20% probability.\n",
    "\n",
    "This explains the use of sigmoid function before the cross-entropy: its goal is to squash the logit to  [0, 1] interval.\n",
    "\n",
    "The formula above still holds for multiple independent features, and that's exactly what tf.nn.sigmoid_cross_entropy_with_logits computes:\n",
    "\n",
    "p = tf.placeholder(tf.float32, shape=[None, 5])\n",
    "logit_q = tf.placeholder(tf.float32, shape=[None, 5])\n",
    "q = tf.nn.sigmoid(logit_q)\n",
    "\n",
    "feed_dict = {\n",
    "  p: [[0, 0, 0, 1, 0],\n",
    "      [1, 0, 0, 0, 0]],\n",
    "  logit_q: [[0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "            [0.3, 0.3, 0.2, 0.1, 0.1]]\n",
    "}\n",
    "\n",
    "prob1 = -p * tf.log(q)\n",
    "prob2 = p * -tf.log(q) + (1 - p) * -tf.log(1 - q)\n",
    "prob3 = p * -tf.log(tf.sigmoid(logit_q)) + (1-p) * -tf.log(1-tf.sigmoid(logit_q))\n",
    "prob4 = tf.nn.sigmoid_cross_entropy_with_logits(labels=p, logits=logit_q)\n",
    "print(prob1.eval(feed_dict))\n",
    "#[ 0.          0.          0.          0.59813893  0.        ]\n",
    "#[ 0.55435514  0.          0.          0.          0.        ]]\n",
    " \n",
    "print(prob2.eval(feed_dict))\n",
    "#[[ 0.79813886  0.79813886  0.79813886  0.59813887  0.79813886]\n",
    "#[ 0.5543552   0.85435522  0.79813886  0.74439669  0.74439669]]\n",
    " \n",
    "print(prob3.eval(feed_dict))\n",
    "#[ 0.7981388   0.7981388   0.7981388   0.59813893  0.7981388 ]\n",
    "#[ 0.55435514  0.85435534  0.7981388   0.74439663  0.74439663]]\n",
    "\n",
    "print(prob4.eval(feed_dict))\n",
    "#[[ 0.7981388   0.7981388   0.7981388   0.59813893  0.7981388 ]\n",
    "#[ 0.55435514  0.85435534  0.7981388   0.74439663  0.74439663]]\n",
    "\n",
    "You should see that the last three tensors are equal, while the prob1 is only a part of cross-entropy, so it contains correct value only when p is 1:\n",
    "\n",
    "Now it should be clear that taking a sum of -p * tf.log(q) along axis=1 doesn't make sense in this setting, though it'd be a valid formula in multi-class case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

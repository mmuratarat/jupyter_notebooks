{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this tutorial\n",
    "1. Learn how to use tf.data and the best practices\n",
    "2. Build an efficient pipeline for loading images and preprocessing them\n",
    "3. Build and efficient pipeline for text, including how to build a vocabulary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I use Tensorflow\n",
    "You use PyTorch\n",
    "Both are great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(\"file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in dataset:\n",
    "    print(line)\n",
    "\n",
    "#We get an error\n",
    "#RuntimeError: dataset.__iter__() is only supported when eager execution is enabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What’s really happening is that dataset is a node of the Tensorflow Graph that contains instructions to read the file. We need to initialize the graph and evaluate this node in a Session if we want to read it. While this may sound awfully complicated, this is quite the oposite : now, even the dataset object is a part of the graph, so you don’t need to worry about how to feed the data into your model !\n",
    "\n",
    "We need to add a few things to make it work. First, let’s create an iterator object over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "#Then you need to call get_next() to get the tensor that will contain your data\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "#The one_shot_iterator method creates an iterator that will be able to iterate once over the dataset. \n",
    "#In other words, once we reach the end of the dataset, it will stop yielding elements and raise an Exception.\n",
    "\n",
    "#Now, next_element is a graph’s node that will contain the next element of iterator over the Dataset at each execution. \n",
    "#Now, let’s run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    for line in range(3):\n",
    "        print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can easily apply transformations to your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, splitting words by space is as easy as adding one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda string: tf.string_split([string]).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for line in range(3):\n",
    "        print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even shuffle the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(buffer_size=3)\n",
    "#It will load elements 3 by 3 and shuffle them at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for line in range(3):\n",
    "        print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and pre-fetch the data (in other words, it will always have one batch ready to be loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and as you can see, we now have a batch created from the shuffled Dataset! All the nodes in the Graph are assumed to be batched: every Tensor will have shape = [None, ...] where None corresponds to the (unspecified) batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why we use initializable iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(\"file.txt\")\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "init_op = iterator.initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the init_op we can chose to “restart” from the beginning. This will become quite handy when we want to perform multiple epochs !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #Initialize the operator\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(next_element))\n",
    "    print(sess.run(next_element))\n",
    "    sess.run(init_op) #Iterator starts from the beginning\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we use only one session over the different epochs, we need to be able to restart the iterator. Some other approaches (like tf.Estimator) alleviate the need of using initializable iterators by creating a new session at each epoch. But this comes at a cost: the weights and the graph must be re-loaded and re-initialized with each call to estimator.train() or estimator.evaluate()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use a Dataset we need three steps:\n",
    "\n",
    "1. **Importing Data** Create a Dataset instance from some data\n",
    "2. **Create an Iterator** By using the created dataset to make an Iterator instance to iterate thought the dataset\n",
    "3. **Consuming Data** By using the created iterator we can get the elements from the dataset to feed the model\n",
    "\n",
    "Regardless of the type of iterator, get_next function of iterator is used to create an operation in your Tensorflow graph which when run over a session, returns the values from the fed Dataset of iterator. Also, iterator doesn’t keep track of how many elements are present in the Dataset. Hence, it is normal to keep running the iterator’s get_next operation till Tensorflow’s tf.errors.OutOfRangeError exception is occurred. This is usually the skeleton code of how a Dataset and iterator looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and perform transformations on it\n",
    "dataset = << Create Dataset object >>\n",
    "dataset = << Perform transformations on dataset >>\n",
    "\n",
    "# Create iterator\n",
    "iterator = << Create iterator using dataset >>\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "# Create session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    try: \n",
    "        # Keep running next_batch till the Dataset is exhausted\n",
    "        while True:\n",
    "            sess.run(next_batch)\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have a numpy array and we want to pass it to tensorflow.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x = np.random.sample((100,2))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "print(dataset.output_types)  # <dtype: 'float64'>\n",
    "print(dataset.output_shapes) #(2,)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also pass more than one numpy array, one classic example is when we have a couple of data divided into features \n",
    "#and labels\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n",
    "print(dataset.output_types)  # tf.float64, tf.float64)\n",
    "print(dataset.output_shapes) #(TensorShape([Dimension(2)]), TensorShape([Dimension(1)]))\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also pass a tensor\n",
    "#Do not forget to use initializable iterator.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([100,2]))\n",
    "print(dataset1.output_types)  # <dtype: 'float32'>\n",
    "print(dataset1.output_shapes) #(2,)\n",
    "\n",
    "iterator = dataset1.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "init_op = iterator.initializer\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also pass a tuple of tensors\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices((tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)) )\n",
    "print(dataset2.output_types)  # (tf.float32, tf.int32)\n",
    "print(dataset2.output_shapes) #TensorShape([]), TensorShape([Dimension(100)]))\n",
    "\n",
    "iterator = dataset2.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "init_op = iterator.initializer\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also pass a nested tuple of tensors\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "dataset3 = tf.data.Dataset.zip((dataset1,dataset2))\n",
    "print(dataset3.output_types)  # (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes) #(TensorShape([Dimension(2)]), (TensorShape([]), TensorShape([Dimension(100)])))\n",
    "\n",
    "iterator = dataset3.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "init_op = iterator.initializer\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is often convenient to give names to each component of an element, for example if they represent different features \n",
    "#of a training example.\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"a\": tf.random_uniform([4]),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also pass a placeholder\n",
    "#This is useful when we want to dynamic change the data inside the Dataset, \n",
    "\n",
    "x=tf.placeholder(tf.float32, shape=[None,2])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "data = np.random.sample((100,2))\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "init_op = iterator.initializer\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op, feed_dict={x:data})\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also initialise a Dataset from a generator, this is useful when we have an array of different elements length \n",
    "#(e.g a sequence)\n",
    "\n",
    "sequence = np.array([[[1]],[[2],[3]],[[3],[4],[5]]])\n",
    "\n",
    "def generator():\n",
    "    for next_element in sequence:\n",
    "        yield next_element\n",
    "        \n",
    "dataset = tf.data.Dataset().batch(1).from_generator(generator, output_types = tf.int64, output_shapes=(tf.TensorShape([None,1])))\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "init_op = iterator.initializer\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(3):\n",
    "        print(sess.run(next_element))\n",
    "\n",
    "#In this case you also need specify the types and the shapes of your data that will be used to create the correct tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ FROM A CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "CSV_path = '/Users/mustafamuratarat/data.csv'\n",
    "dataset = tf.contrib.data.make_csv_dataset(CSV_path, batch_size = 15)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element =  iterator.get_next()\n",
    "print(next_element)\n",
    "inputs, labels = next_element['Estimation'], next_element['Algorithm']\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([inputs, labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "# Create a dataset with data of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "dataset = dataset.repeat(2)\n",
    "# Duplicate the dataset\n",
    "# Data will be [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "for i in iterator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "dataset = dataset.shuffle(5)\n",
    "# Shuffle the dataset\n",
    "# Assumed shuffling: [3, 0, 7, 9, 4, 2, 5, 0, 1, 7, 5, 9, 4, 6, 2, 8, 6, 8, 1, 3]\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "for i in iterator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In Map transformation, you can apply some operations to all the individual data elements in your dataset. \n",
    "#Use this particular transformation to apply various types of data augmentation\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "\n",
    "def map_fn(x):\n",
    "    return x * 3\n",
    "\n",
    "dataset = dataset.map(map_fn)\n",
    "# Same as dataset = dataset.map(lambda x: x * 3)\n",
    "# Multiply each element with 3 using map transformation\n",
    "# Dataset: [0, 3, 6, 9, 12, 15, 18, 21, 24, 27]\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "for i in iterator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#During the course of training, if you wish to filter out some elements from Dataset, use filter function.\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "\n",
    "def filter_fn(x):\n",
    "    return tf.reshape(tf.not_equal(x % 5, 1), [])\n",
    "\n",
    "dataset = dataset.filter(filter_fn)\n",
    "# Same as dataset = dataset.filter(lambda x: tf.reshape(tf.not_equal(x % 5, 1), []))\n",
    "# Filter out all those elements whose modulus 5 returns 1\n",
    "# Dataset: [0, 2, 3, 4, 5, 7, 8, 9]\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "for i in iterator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "dataset = dataset.batch(4)\n",
    "\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "for i in iterator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordering of Transformation\n",
    "\n",
    "The ordering of the application of the transformation is very important. Your model may learn differently for the same Dataset but differently ordered transformations. Take a look at the code sample in which it has been shown that different set of data is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordering #1\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "# Dataset: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "dataset1 = dataset1.batch(4)\n",
    "# Dataset: [0, 1, 2, 3], [4, 5, 6, 7], [8, 9]\n",
    "\n",
    "dataset1 = dataset1.repeat(2)\n",
    "# Dataset: [0, 1, 2, 3], [4, 5, 6, 7], [8, 9], [0, 1, 2, 3], [4, 5, 6, 7], [8, 9]\n",
    "# Notice a 2 element batch in between\n",
    "\n",
    "dataset1 = dataset1.shuffle(4)\n",
    "# Shuffles at batch level.\n",
    "# Dataset: [0, 1, 2, 3], [4, 5, 6, 7], [8, 9], [8, 9], [0, 1, 2, 3], [4, 5, 6, 7]\n",
    "\n",
    "\n",
    "\n",
    "# Ordering #2\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "# Dataset: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "dataset2 = dataset2.shuffle(4)\n",
    "# Dataset: [3, 1, 0, 4, 5, 8, 6, 9, 7, 2]\n",
    "\n",
    "dataset2 = dataset2.repeat(2)\n",
    "# Dataset: [3, 1, 0, 4, 5, 8, 6, 9, 7, 2, 3, 1, 0, 4, 5, 8, 6, 9, 7, 2]\n",
    "\n",
    "dataset2 = dataset2.batch(4)\n",
    "# Dataset: [3, 1, 0, 4], [5, 8, 6, 9], [7, 2, 3, 1], [0, 4, 5, 8], [6, 9, 7, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exists 4 types of iterators.\n",
    "1. **One shot**. It can iterate once through a dataset, you cannot feed any value to it.\n",
    "2. **Initializable** You can dynamically change calling its initializer operation and passing the new data with feed_dict. It is basically a bucket that you can fill with stuff.\n",
    "3. **Reinitializable**. It can be initialised from different Dataset. Very useful when you have a training dataset that needs some additional transformation and a testing dataset. It is like using a tower crane to select different container.\n",
    "4. **Feedable**. It can be used to select with iterator to use. Following the previous example, it is like a tower crane that selects which tower crane to use to select which container to take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Shot Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x=np.random.sample((100,2))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializable Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape =[None,2])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "data = np.random.sample((100,2))\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "init_op = iterator.initializer\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op, feed_dict ={x:data})\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that now we have a train set and a test set, a real common scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#initializable iterator to switch between dataset\n",
    "epochs = 10\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape =[None,2]), tf.placeholder(tf.float32, shape =[None,1])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.array([[1,2]]), np.array([[0]]))\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "features, labels = iterator.get_next()\n",
    "init_op = iterator.initializer\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #initialise iterator with train data\n",
    "    sess.run(init_op, feed_dict ={x:train_data[0], y: train_data[1]})\n",
    "    for _ in range(epochs):\n",
    "        print(sess.run([features, labels]))\n",
    "    #switch to test data\n",
    "    sess.run(init_op, feed_dict ={x:test_data[0], y: test_data[1]})\n",
    "    print(sess.run([features, labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, using initializer operation, we have changed the dataset between training and test using the same Dataset object.\n",
    "\n",
    "This iterator is very ideal when you have to train your model with datasets which are split across multiple places and you are not able to accumulate them into one place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinitializable Iterator\n",
    "\n",
    "The concept is similar to before, we want to dynamic switch between data. But instead of feed new data to the same dataset, we switch dataset. As before, we want to have a train dataset and a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "#create some data\n",
    "training_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "testing_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
    "\n",
    "#create two datasets, one for training and one for testing\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(training_data)\n",
    "testing_dataset = tf.data.Dataset.from_tensor_slices(testing_data)\n",
    "\n",
    "#this is the trick, we create a generic iterator\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)\n",
    "\n",
    "#create initialization operations\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "testing_init_op = iterator.make_initializer(testing_dataset)\n",
    "\n",
    "features, labels = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(training_init_op) #swtich to train dataset\n",
    "    for _ in range(epochs):\n",
    "        print(sess.run([features, labels]))\n",
    "    sess.run(testing_init_op) #swtich to testing dataset\n",
    "    print(sess.run([features, labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedable Iterator\n",
    "This is very similar to the reinitializable iterator, but instead of switch between datasets, it switch between iterators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "#create some data\n",
    "training_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "testing_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
    "\n",
    "#create placeholder\n",
    "x, y = tf.placeholder(tf.float32, shape =[None,2]), tf.placeholder(tf.float32, shape =[None,1])\n",
    "\n",
    "#create two datasets, one for training and one for testing\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "testing_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "\n",
    "#Then, we can create our iterator, in this case we use the initializable iterator, \n",
    "#but you can also use a one shot iterator\n",
    "training_iterator = training_dataset.make_initializable_iterator()\n",
    "testing_iterator = testing_dataset.make_initializable_iterator()\n",
    "\n",
    "#Now, we need to defined and handle , that will be out placeholder that can be dynamically changed.\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "\n",
    "#Then, similar to before, we define a generic iterator using the shape of the dataset\n",
    "iterator = tf.data.Iterator.from_string_handle(handle, training_dataset.output_types, training_dataset.output_shapes)\n",
    "\n",
    "#Then, we get the next elements\n",
    "next_elements = iterator.get_next()\n",
    "\n",
    "#create initialization operations\n",
    "training_init_op = training_iterator.initializer\n",
    "testing_init_op = testing_iterator.initializer\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_handle = sess.run(training_iterator.string_handle())\n",
    "    test_handle = sess.run(testing_iterator.string_handle())\n",
    "    #initialize iterators\n",
    "    sess.run(training_init_op, feed_dict ={x:training_data[0], y: training_data[1]})\n",
    "    sess.run(testing_init_op, feed_dict ={x:testing_data[0], y: testing_data[1]})\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        x,y = sess.run(next_elements, feed_dict = {handle: train_handle})\n",
    "        print(x,y)\n",
    "    x,y = sess.run(next_elements, feed_dict = {handle: test_handle})\n",
    "    print([x,y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consuming Data\n",
    "\n",
    "In order to pass the data to a model we have to just pass the tensors generated from get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "#using two numpy arrays\n",
    "features, labels = (np.array([np.random.sample((100,2))]), np.array([np.random.sample((100,1))]))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(batch_size)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "x, y = iterator.get_next()\n",
    "\n",
    "#make a simple neural network model\n",
    "net = tf.layers.dense(x, 8, activation = tf.tanh)\n",
    "net = tf.layers.dense(net, 8, activation = tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation = tf.tanh)\n",
    "loss = tf.losses.mean_squared_error(labels = y, predictions=prediction)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(epochs):\n",
    "        _, loss_value = sess.run([train_op, loss])\n",
    "        #data (100X2, 100X1) will pass two times for one epoch\n",
    "        print(sess.run([x,y]))\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#Switch between train and test set using Initializable iterator\n",
    "EPOCHS = 10\n",
    "#create a placeholder to dynamically switch between batch sizes\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat()\n",
    "\n",
    "# using two numpy arrays\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((20,2)), np.random.sample((20,1)))\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "features, labels = iterator.get_next()\n",
    "init_op = iterator.initializer\n",
    "# make a simple model\n",
    "net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "n_batches = train_data[0].shape[0] // BATCH_SIZE\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #initialise iterator with train data\n",
    "    sess.run(init_op, feed_dict={x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})\n",
    "    print('Training...')\n",
    "    for i in range(EPOCHS):\n",
    "        total_loss=0\n",
    "        for _ in range(n_batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            #print(sess.run([features, labels]))\n",
    "            total_loss += loss_value\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, total_loss / n_batches))\n",
    "    #initialise iterator with test data\n",
    "    sess.run(init_op, feed_dict={x: test_data[0], y:test_data[1], batch_size: test_data[0].shape[0]})\n",
    "    print(\"Test: {:.4f}\".format(sess.run(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping all together -> Switch between train and test set using Reinitializable iterator\n",
    "EPOCHS = 10\n",
    "# create a placeholder to dynamically switch between batch sizes\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size).repeat()\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size) # always batch even if you want to one shot it\n",
    "# using two numpy arrays\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((20,2)), np.random.sample((20,1)))\n",
    "\n",
    "# create a iterator of the correct shape and type\n",
    "iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "features, labels = iter.get_next()\n",
    "# create the initialisation operations\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "# make a simple model\n",
    "net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "n_batches = train_data[0].shape[0] // 16\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # initialise iterator with train data\n",
    "    sess.run(train_init_op, feed_dict = {x : train_data[0], y: train_data[1], batch_size: 16})\n",
    "    print('Training...')\n",
    "    for i in range(EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(n_batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "    # initialise iterator with test data\n",
    "    sess.run(test_init_op, feed_dict = {x : test_data[0], y: test_data[1], batch_size:len(test_data[0])})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time = {}\n",
    "# copied form https://medium.com/pythonhive/python-decorator-to-measure-the-execution-time-of-methods-fa04cb6bb36d\n",
    "def how_much(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        \n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__)\n",
    "            kw['log_time'][name] = (te - ts)\n",
    "            \n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark\n",
    "import time\n",
    "DATA_SIZE = 5000\n",
    "DATA_SHAPE = ((32,32),(20,))\n",
    "BATCH_SIZE = 64 \n",
    "N_BATCHES = DATA_SIZE // BATCH_SIZE\n",
    "EPOCHS = 10\n",
    "\n",
    "test_size = (DATA_SIZE//100)*20 \n",
    "\n",
    "train_shape = ((DATA_SIZE, *DATA_SHAPE[0]),(DATA_SIZE, *DATA_SHAPE[1]))\n",
    "test_shape = ((test_size, *DATA_SHAPE[0]),(test_size, *DATA_SHAPE[1]))\n",
    "print(train_shape, test_shape)\n",
    "train_data = (np.random.sample(train_shape[0]), np.random.sample(train_shape[1]))\n",
    "test_data = (np.random.sample(test_shape[0]), np.random.sample(test_shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to keep track of the methodds\n",
    "log_time = {}\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "input_shape = [None, *DATA_SHAPE[0]] # [None, 64, 64, 3]\n",
    "output_shape = [None,*DATA_SHAPE[1]] # [None, 20]\n",
    "print(input_shape, output_shape)\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=input_shape), tf.placeholder(tf.float32, shape=output_shape)\n",
    "\n",
    "@how_much\n",
    "def one_shot(**kwargs):\n",
    "    print('one_shot')\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(BATCH_SIZE).repeat()\n",
    "    train_el = train_dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(BATCH_SIZE).repeat()\n",
    "    test_el = test_dataset.make_one_shot_iterator().get_next()\n",
    "    for i in range(EPOCHS):\n",
    "        print(i)\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(train_el)\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(test_el)\n",
    "            \n",
    "@how_much\n",
    "def initialisable(**kwargs):\n",
    "    print('initialisable')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "    iter = dataset.make_initializable_iterator()\n",
    "    elements = iter.get_next()\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        print(i)\n",
    "        sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements)\n",
    "        sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements)\n",
    "@how_much            \n",
    "def reinitializable(**kwargs):\n",
    "    print('reinitializable')\n",
    "    # create two datasets, one for training and one for test\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(BATCH_SIZE).repeat()\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(BATCH_SIZE).repeat()\n",
    "    # create a iterator of the correct shape and type\n",
    "    iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "    elements = iter.get_next()\n",
    "    # create the initialisation operations\n",
    "    train_init_op = iter.make_initializer(train_dataset)\n",
    "    test_init_op = iter.make_initializer(test_dataset)\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        print(i)\n",
    "        sess.run(train_init_op, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements)\n",
    "        sess.run(test_init_op, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements)\n",
    "            \n",
    "@how_much            \n",
    "def feedable(**kwargs):\n",
    "    print('feedable')\n",
    "    # create two datasets, one for training and one for test\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(BATCH_SIZE).repeat()\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(BATCH_SIZE).repeat()\n",
    "    # create the iterators from the dataset\n",
    "    train_iterator = train_dataset.make_initializable_iterator()\n",
    "    test_iterator = test_dataset.make_initializable_iterator()\n",
    "\n",
    "    handle = tf.placeholder(tf.string, shape=[])\n",
    "    iter = tf.data.Iterator.from_string_handle(\n",
    "        handle, train_dataset.output_types, train_dataset.output_shapes)\n",
    "    elements = iter.get_next()\n",
    "\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    test_handle = sess.run(test_iterator.string_handle())\n",
    "\n",
    "    sess.run(train_iterator.initializer, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "    sess.run(test_iterator.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        print(i)\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements, feed_dict={handle: train_handle})\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements, feed_dict={handle: test_handle})\n",
    "            \n",
    "one_shot(log_time=log_time)\n",
    "initialisable(log_time=log_time)\n",
    "reinitializable(log_time=log_time)\n",
    "feedable(log_time=log_time)\n",
    "\n",
    "sorted((value,key) for (key,value) in log_time.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-armed bandit problem \n",
    "# Page 32: A Simple Bandit Algorithm of the book \"Introduction to Reinforcement Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from math import sqrt, log\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_problem(k):\n",
    "    return np.random.normal(loc=0.0, scale=1, size=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reward(problem, action):\n",
    "    return np.random.normal(loc=problem[action], scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_bandit(problem, k, steps, exploration_rate):\n",
    "    Q = {i: 0 for i in range(k)} # 1. Value function\n",
    "    N = {i: 0 for i in range(k)} # 2. Number of actions, for update rule\n",
    "    \n",
    "    for i in range(steps): # 3. Main loop\n",
    "        explore = random.uniform(0, 1) < exploration_rate \n",
    "        if explore:\n",
    "            action = random.randint(0, k - 1) # 5. Exploration: Choosing random action\n",
    "        else:\n",
    "            action = max(Q, key=Q.get) # 6. Choose action with maximum mean reward\n",
    "            \n",
    "        reward = generate_reward(problem, action) # 7. Get reward for current action\n",
    "        N[action] += 1 # 8. Update action number\n",
    "        Q[action] += (1 / N[action]) * (reward - Q[action]) # 9. Update value dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 of Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a multi-arm bandit problem with $k = 5$ actions, denoted 1, 2, 3, 4, and 5. Consider\n",
    "applying to this problem a bandit algorithm using \\varepsilon-greedy action selection, sample-average\n",
    "action-value estimates, and initial estimates of $Q_{1}(a)$ = 0 for all $a$. Suppose the initial\n",
    "sequence of actions and rewards is $A_{1} = 1$, $R_{1} = –2$, $A_{2} = 2$, $R_{2} = 3$, $A_{3} = 3$, $R_{3} = –1$, $A_{4} =2$, $R_4 = 2$, $A_{5} = 3$, $R_{5} = 0$, $A_{6} = 4$, $R_{6} = 5$. On some of these time steps the \u000f case may have\n",
    "occurred causing an action to be selected at random (assume that when the $\\varepsilon$ case occurs,\n",
    "the arm(s) with the max value is excluded from the random selection).\n",
    "\n",
    "(a) On which time steps did this definitely occur?\n",
    "\n",
    "(b) On which time steps could this possibly have occurred?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [-2. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [-2. ,  3. ,  0. ,  0. ,  0. ],\n",
       "       [-2. ,  3. , -1. ,  0. ,  0. ],\n",
       "       [-2. ,  2.5, -1. ,  0. ,  0. ],\n",
       "       [-2. ,  2.5, -0.5,  0. ,  0. ],\n",
       "       [-2. ,  2.5, -0.5,  5. ,  0. ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = np.array([-2,3,-1,2,0,5])\n",
    "A = np.array([1,2,3,2,3,4])\n",
    "\n",
    "actions = 5\n",
    "iterations = len(A)\n",
    "table = np.zeros((iterations + 1, actions))\n",
    "\n",
    "def weird_division(n, d):\n",
    "    return n / d if d else 0\n",
    "\n",
    "table[0,:] = np.zeros(actions)\n",
    "table[1,:] = np.array([1,0,0,0,0])\n",
    "for t in range(1,iterations+1):\n",
    "    for a in range(1, actions+1):\n",
    "        table[t,a-1]= weird_division(np.sum(R[0:t] * np.piecewise(A, [A == a, A != a], [1, 0])[0:t]),np.sum(np.piecewise(A, [A == a, A != a], [1, 0])[0:t]))\n",
    "        \n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Bandit Example of the book \"Introduction to Reinforcement Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        , 0.        ],\n",
       "       [1.        , 1.        , 0.        , 0.        ],\n",
       "       [1.        , 1.5       , 0.        , 0.        ],\n",
       "       [1.        , 1.66666667, 0.        , 0.        ],\n",
       "       [1.        , 1.66666667, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = np.array([1,1,2,2,0])\n",
    "A = np.array([1,2,2,2,3])\n",
    "\n",
    "actions = 4\n",
    "iterations = len(A)\n",
    "table = np.zeros((iterations + 1, actions))\n",
    "\n",
    "def weird_division(n, d):\n",
    "    return n / d if d else 0\n",
    "\n",
    "table[0,:] = np.zeros(actions)\n",
    "table[1,:] = np.array([1,0,0,0])\n",
    "for t in range(1,iterations+1):\n",
    "    for a in range(1, actions+1):\n",
    "        table[t,a-1]= weird_division(np.sum(R[0:t] * np.piecewise(A, [A == a, A != a], [1, 0])[0:t]),np.sum(np.piecewise(A, [A == a, A != a], [1, 0])[0:t]))\n",
    "        \n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 of Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem will require you to use equation 2.5 from the book to examine the effects of\n",
    "the step size α on the performance of the estimate. Assume our initial estimate is 0.\n",
    "\n",
    "- Write a simple python script which computes the most recent update for the following\n",
    "sequences of rewards using three different values for $\\alpha$ ($\\alpha = 0.5, 0.25, 1$). Submit the\n",
    "printed results.\n",
    "\n",
    "| step 1 2 3 4 5 6 7 8 9 10    \t|\n",
    "|------------------------------\t|\n",
    "| series 1 2 2 2 2 2 2 2 2 2 2 \t|\n",
    "| series 2 2 2 2 2 2 4 4 4 4 4 \t|\n",
    "| series 3 2 4 2 4 2 4 2 4 2 4 \t|\n",
    "\n",
    "- Which step size worked best for which cases? Explain and generalize this for different\n",
    "types of reward distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 4.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 4.]\n",
      " [2. 2. 2.]\n",
      " [2. 4. 4.]\n",
      " [2. 4. 2.]\n",
      " [2. 4. 4.]\n",
      " [2. 4. 2.]\n",
      " [2. 4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "R = np.array([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2],[2, 2, 2, 2, 2, 4, 4, 4, 4, 4],[2, 4, 2, 4, 2, 4, 2, 4, 2, 4]]).T\n",
    "\n",
    "step_size = 1\n",
    "steps = 10\n",
    "series = 3\n",
    "\n",
    "Q = np.zeros((steps+1, series))\n",
    "\n",
    "Q[0,:] = np.zeros(series)\n",
    "for n in range(10):\n",
    "    Q[n+1,:] = Q[n,:] + step_size * (R[n,:] - Q[n,:])\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM 1 OF ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving linear equations simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0.1875, 0.25, 0.291666666666667, 0.270833333333333)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import *\n",
    "from sympy.solvers.solveset import linsolve\n",
    "\n",
    "pi0, pi1, pi2, pi3 = symbols('pi0, pi1, pi2, pi3')\n",
    "M = Matrix(((-0.75, 0, 0.25, 0.25, 0),(0.25, -0.75, 0.25, 0.25, 0),(0.5, 0.5, -0.75, 0, 0),(0, 0.25, 0.25, -0.5, 0),(1,1,1,1,1)))\n",
    "system = A, b = M[:, :-1], M[:, -1]\n",
    "linsolve(system, pi0, pi1, pi2, pi3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

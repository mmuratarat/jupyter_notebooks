{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-directional Recurrent Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create input data\n",
    "#2 instances with 10 times steps and 8 inputs (features)\n",
    "X = np.random.randn(2, 10, 8)\n",
    "\n",
    "# The second instance is of length 6 \n",
    "# variable length of sequences\n",
    "X[1,6:] = 0\n",
    "X_lengths = [10, 6]\n",
    "\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=64, state_is_tuple=True)\n",
    "\n",
    "outputs, states  = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw=cell,\n",
    "    cell_bw=cell,\n",
    "    dtype=tf.float64,\n",
    "    sequence_length=X_lengths,\n",
    "    inputs=X)\n",
    "\n",
    "#outputs will be a tuple consisting of outputs of forward and backward LSTM layers. \n",
    "#Each of these outputs has shape [batch_size X time_steps X num_neurons]\n",
    "output_fw, output_bw = outputs\n",
    "\n",
    "#states will be a tuple consisting of states of forward and backward LSTM layers. \n",
    "#Each of these states has shape [batch_size X num_neurons]\n",
    "states_fw, states_bw = states\n",
    "\n",
    "result = tf.contrib.learn.run_n(\n",
    "    {\"outputs\": outputs, \"output_fw\": output_fw, \"output_bw\": output_bw, \"states_fw\": states_fw, \"states_bw\": states_bw},\n",
    "    n=1,\n",
    "    feed_dict=None)\n",
    "print(result[0][\"outputs\"])\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(result[0][\"output_fw\"])\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(result[0][\"output_bw\"])\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(result[0][\"output_fw\"].shape)\n",
    "#(2, 10, 64)\n",
    "print(result[0][\"output_bw\"].shape)\n",
    "#(2, 10, 64)\n",
    "print(result[0][\"states_fw\"].h.shape)\n",
    "#(2, 64)\n",
    "print(result[0][\"states_bw\"].h.shape)\n",
    "#(2, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Network Parameters\n",
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "num_hidden = 5\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "\n",
    "lstm_fw_cell = tf.contrib.rnn.LSTMCell(num_hidden)\n",
    "lstm_bw_cell = tf.contrib.rnn.LSTMCell(num_hidden)\n",
    "outputs, states = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, X, dtype=tf.float32)\n",
    "\n",
    "X_batch = np.array([ \n",
    "    [[0,1,2],[9,8,7]],#instance 0\n",
    "    [[3,4,5],[0,0,0]],#instance 1\n",
    "    [[6,7,8],[6,5,4]],#instance 2\n",
    "    [[9,0,1],[3,2,1]]])#instance 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_val, states_val = sess.run([outputs, states], feed_dict={X: X_batch})\n",
    "    \n",
    "states_val #consisting of c and h matrices.\n",
    "#A tuple (output_state_fw, output_state_bw) containing the forward and the backward final states of bidirectional rnn.\n",
    "\n",
    "outputs_val #A tuple (output_fw, output_bw) containing the forward and the backward rnn output\n",
    "output_fw, output_bw = outputs_val\n",
    "\n",
    "#output_fw is the same with outputs_val[0]\n",
    "#output_bw is the same with outputs_val[1]\n",
    "\n",
    "print(output_fw.shape)\n",
    "print(output_bw.shape)\n",
    "print(outputs_val[0].shape) #output_fw (4, 2, 5)\n",
    "print(outputs_val[1].shape) #output_bw (4, 2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Batch size = 2, sequence length = 3, number features = 1, shape=(2, 3, 1)\n",
    "values231 = np.array([\n",
    "    [[1], [2], [3]],\n",
    "    [[2], [3], [4]]])\n",
    "\n",
    "# Batch size = 3, sequence length = 5, number features = 2, shape=(3, 5, 2)\n",
    "#values352 = np.array([\n",
    "#    [[1, 4], [2, 5], [3, 6], [4, 7], [5, 8]],\n",
    "#    [[2, 5], [3, 6], [4, 7], [5, 8], [6, 9]],\n",
    "#    [[3, 6], [4, 7], [5, 8], [6, 9], [7, 10]]\n",
    "#])\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_values231 = tf.constant(values231, dtype=tf.float32)\n",
    "\n",
    "lstm_cell_fw = tf.contrib.rnn.LSTMCell(100)\n",
    "lstm_cell_bw = tf.contrib.rnn.LSTMCell(105) # change to 105 just so can see the effect in output\n",
    "\n",
    "(output_fw, output_bw), (output_state_fw, output_state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw=lstm_cell_fw, \n",
    "    cell_bw=lstm_cell_bw, \n",
    "    inputs=tf_values231,\n",
    "    dtype=tf.float32)\n",
    "    \n",
    "print(output_fw)\n",
    "# tf.Tensor 'bidirectional_rnn/fw/fw/transpose:0' shape=(2, 3, 100) dtype=float32\n",
    "print(output_bw)\n",
    "# tf.Tensor 'ReverseV2:0' shape=(2, 3, 105) dtype=float32\n",
    "print(output_state_fw.c)\n",
    "# tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(2, 100) dtype=float32\n",
    "print(output_state_fw.h)\n",
    "# tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(2, 100) dtype=float32\n",
    "print(output_state_bw.c)\n",
    "# tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(2, 105) dtype=float32\n",
    "print(output_state_bw.h)\n",
    "# tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(2, 105) dtype=float32\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_run, state_run = sess.run([(output_fw, output_bw), (output_state_fw, output_state_bw)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## static_bidirectional_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "tf.logging.set_verbosity(old_v)\n",
    "\n",
    "#This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples \n",
    "#for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values \n",
    "#from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
    "#MNIST image shape is 28*28px, we will then handle 28 sequences of 28 timesteps for every sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, timesteps, num_input], name='input_placeholder')\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes], name='labels_placeholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
    "    'out': tf.Variable(tf.random_normal([2*num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we are receiving inputs of shape [batch_size,time_steps,n_input],we need to convert it into a list of \n",
    "#tensors of shape [batch_size,n_inputs] of length time_steps so that it can be then fed to static_rnn.\n",
    "\n",
    "#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\n",
    "input = tf.unstack(X, timesteps, 1) \n",
    "\n",
    "#Define lstm cells with tensorflow\n",
    "# Forward direction cell\n",
    "lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "# Backward direction cell\n",
    "lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "# Get lstm cell output\n",
    "outputs, output_state_fw, output_state_bw = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, input, dtype=tf.float32)\n",
    "# Linear activation, using rnn inner loop last output\n",
    "logits = tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    for step in range(1, training_steps+1):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Reshape data to get 28 seq of 28 elements\n",
    "            batch_x = batch_x.reshape((-1, timesteps, num_input))\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayered Bidirectional LSTM using nn.bidirectional_dynamic_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Picture1.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 28, 256)\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 # timesteps\n",
    "rnn_size = 128 # hidden layer num of features\n",
    "num_layers = 2\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, timesteps, num_input], name='input_placeholder')\n",
    "        \n",
    "def bidirectional_lstm(X, num_layers, rnn_size):\n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer),reuse=tf.AUTO_REUSE):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.truncated_normal_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.truncated_normal_initializer(-0.1, 0.1, seed=2))\n",
    "            \n",
    "            (forward_output, backward_output), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw,cell_bw, X, dtype=tf.float32)\n",
    "            output = tf.concat(values = [forward_output, backward_output], axis=2)\n",
    "    return output\n",
    "\n",
    "output = bidirectional_lstm(X, num_layers, rnn_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "X_batch = np.random.rand(2, timesteps, num_input)\n",
    "\n",
    "with  tf.Session() as sess:\n",
    "    init.run()\n",
    "    output_val = sess.run(output, feed_dict={X: X_batch})\n",
    "\n",
    "print(output_val.shape)\n",
    "#(2, 28, 256)\n",
    "#After concatenation, the output will consist of the output of the last forward layer AND \n",
    "#of the output of the last backward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hand-written] accuracy: 0.36461818157271897\n",
      "[tf.metrics.accuracy] accuracy: 0.364618182182312\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "num_steps=28\n",
    "num_inputs = 28\n",
    "num_classes = 10\n",
    "num_neurons = 128\n",
    "num_layers = 3\n",
    "batch_size = 500\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"graph_inputs\"):\n",
    "        X = tf.placeholder(tf.float32, [None, num_steps, num_inputs], name='input_placeholder')\n",
    "        y = tf.placeholder(tf.float32, [None, num_classes], name='labels_placeholder')\n",
    "        output_keep_prob = tf.placeholder_with_default(1.0, shape=(), name =\"output_dropout\")\n",
    "\n",
    "    def build_lstm_cell(num_neurons, output_keep_prob):\n",
    "        \"\"\"Returns a dropout-wrapped LSTM-cell.\n",
    "        See https://stackoverflow.com/a/44882273/2628369 for why this local function is necessary.\n",
    "        Returns:\n",
    "        tf.contrib.rnn.DropoutWrapper: The dropout-wrapped LSTM cell.\n",
    "        \"\"\"\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(num_units=num_neurons, initializer=initializer, forget_bias=1.0, state_is_tuple=True, name='LSTM_cell')\n",
    "        lstm_cell_drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=output_keep_prob)\n",
    "        return lstm_cell_drop\n",
    "    \n",
    "    with tf.name_scope(\"LSTM\"):\n",
    "        with tf.name_scope(\"Cell\"):\n",
    "            multi_layer_cell = tf.contrib.rnn.MultiRNNCell([build_lstm_cell(num_neurons, output_keep_prob) for _ in range(num_layers)], state_is_tuple=True)\n",
    "        with tf.name_scope(\"Model\"):\n",
    "            outputs, states = tf.nn.dynamic_rnn(cell=multi_layer_cell, inputs=X, swap_memory=False, time_major = False, dtype=tf.float32)#[Batch_size, time_steps, num_neurons]\n",
    "        with tf.name_scope(\"Graph_Outputs\"):\n",
    "            outputs = tf.transpose(outputs, [1, 0, 2]) # [num_timesteps, batch_size, num_neurons]\n",
    "            outputs = tf.gather(outputs, int(outputs.get_shape()[0]) - 1) # [batch_size, num_neurons]\n",
    "        with tf.variable_scope('Softmax'):\n",
    "            logits =  tf.layers.dense(inputs = outputs, units = num_classes, name=\"logits\") #[Batch_size, num_classes]\n",
    "        with tf.name_scope('Predictions'):\n",
    "            predictions = tf.nn.softmax(logits, name=\"predictions\")  #[Batch_size, num_classes]\n",
    "        with tf.name_scope('Accuracy1'):\n",
    "            correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\n",
    "            accuracy1 = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "        with tf.name_scope('Accuracy2'):\n",
    "            accuracy2, accuracy_op = tf.metrics.accuracy(labels=tf.argmax(y, 1), predictions=tf.argmax(predictions, 1))\n",
    "        with tf.name_scope('Loss'):\n",
    "            xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y)\n",
    "            loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        with tf.name_scope('Train'):\n",
    "            optimizer= tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "            trainer=optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "                        \n",
    "#Batch accuracy\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    total_batch = mnist.train.num_examples // batch_size\n",
    "    for batch in range(total_batch):\n",
    "        tf.local_variables_initializer().run()\n",
    "        xBatch, yBatch = mnist.train.next_batch(batch_size)\n",
    "        xBatch = xBatch.reshape((batch_size, num_steps, num_inputs))\n",
    "        sess.run(trainer, feed_dict={X: xBatch, y: yBatch, output_keep_prob: 0.5})\n",
    "        miniBatchAccuracy1 = sess.run(accuracy1, feed_dict={X: xBatch, y: yBatch, output_keep_prob: 0.5})\n",
    "        print('[hand-written] Batch {} accuracy: {}'.format(batch, miniBatchAccuracy1))\n",
    "        accuracy_op_val = sess.run(accuracy_op, feed_dict={X: xBatch, y: yBatch, output_keep_prob: 0.5})\n",
    "        miniBatchAccuracy2 = sess.run(accuracy2)\n",
    "        print(\"[tf.metrics.accuracy] Batch {} accuracy: {}\".format(batch, miniBatchAccuracy2))\n",
    "    sess.close()\n",
    "\n",
    "#overall accuracy, one batch at a time\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    tf.local_variables_initializer().run()\n",
    "    avg_accuracy=0\n",
    "    total_batch = mnist.train.num_examples // batch_size\n",
    "    for batch in range(total_batch):\n",
    "        xBatch, yBatch = mnist.train.next_batch(batch_size)\n",
    "        xBatch = xBatch.reshape((batch_size, num_steps, num_inputs))\n",
    "        sess.run(trainer, feed_dict={X: xBatch, y: yBatch, output_keep_prob: 1})\n",
    "        miniBatchAccuracy1 = sess.run(accuracy1, feed_dict={X: xBatch, y: yBatch, output_keep_prob: 1})\n",
    "        avg_accuracy += miniBatchAccuracy1 / total_batch\n",
    "        \n",
    "        accuracy_op_val = sess.run(accuracy_op, feed_dict={X: xBatch, y: yBatch, output_keep_prob: 1})\n",
    "    miniBatchAccuracy2 = sess.run(accuracy2)    \n",
    "        \n",
    "        \n",
    "    print('[hand-written] accuracy: {}'.format(avg_accuracy))\n",
    "    print(\"[tf.metrics.accuracy] accuracy: {}\".format(miniBatchAccuracy2))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
